# -*- coding: utf-8 -*-
"""
åˆå¹¶è„šæœ¬ï¼šåŒèŠ±é¡ºæ•°æ®è‡ªåŠ¨ä¸‹è½½ä¸æ‰€å±æ¦‚å¿µæ›´æ–°
é€‚ç”¨ç¯å¢ƒï¼šGitHub Actions / æœ¬åœ°è‡ªåŠ¨åŒ–ä»»åŠ¡
ä¼˜åŒ–ï¼šä»…ä¿å­˜å¿…è¦åˆ—ï¼Œå¤§å¹…å‡å°‘CSVä½“ç§¯
"""

import os
import sys
import time
import random
import glob
import re
import pandas as pd
import pywencai
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor

# =====================================================================
# --- 0. å…¨å±€è·¯å¾„ä¸ç¯å¢ƒé…ç½® ---
# =====================================================================

from modules.config import CONCEPT_PATH, CALENDAR_PATH, DOWNLOAD_CONFIGS,THS_DATA_ROOT
# =====================================================================
# --- I. ä¸‹è½½é…ç½®éƒ¨åˆ† (æ–°å¢ keep_cols) ---
# =====================================================================
# æ³¨æ„ï¼špywencai è¿”å›çš„åˆ—åå¯èƒ½åŒ…å« "code", "è‚¡ç¥¨ä»£ç ", "è‚¡ç¥¨åç§°" ç­‰
DOWNLOAD_CONFIGS = {
    'æ”¶ç›˜æ•°æ®': {
        'backup_dir': os.path.join(THS_DATA_ROOT, 'æ”¶ç›˜'),
        'max_threads': 2,
        'question_suffix': 'æ‰€å±è¡Œä¸š',  # åªéœ€è¦è¡Œä¸šï¼Œé—®å¥ç®€åŒ–
        'data_threshold': 3000, 
        'query_delay_range': (3, 6),
        # åªä¿ç•™ä»£ç ã€åç§°å’Œè¡Œä¸š
        'keep_cols': ['è‚¡ç¥¨ä»£ç ', 'code', 'è‚¡ç¥¨åç§°', 'æ‰€å±åŒèŠ±é¡ºè¡Œä¸š']
    },
    'æ¶¨è·Œåœæ•°æ®': {
        'backup_dir': os.path.join(THS_DATA_ROOT, 'æ¶¨åœ'),
        'max_threads': 4,
        'question_suffix': 'æ¶¨è·Œåœ',
        'data_threshold': 0, 
        'query_delay_range': (3, 6),
        # åªä¿ç•™ä»£ç ã€åç§°å’ŒåŸå› 
        'keep_cols': ['è‚¡ç¥¨ä»£ç ', 'code', 'è‚¡ç¥¨åç§°', 'æ¶¨åœåŸå› ç±»åˆ«']
    },
    'æ‰€å±æ¦‚å¿µ': {
        'backup_dir': os.path.join(THS_DATA_ROOT, 'æ‰€å±æ¦‚å¿µ'),
        'max_threads': 2,
        'question_suffix': 'æ‰€å±æ¦‚å¿µ',
        'data_threshold': 3000,
        'query_delay_range': (3, 6),
        # åªä¿ç•™ä»£ç ã€åç§°å’Œæ¦‚å¿µ
        'keep_cols': ['è‚¡ç¥¨ä»£ç ', 'code', 'è‚¡ç¥¨åç§°', 'æ‰€å±æ¦‚å¿µ']
    }
}

SCENARIO_ORDER = ['æ”¶ç›˜æ•°æ®', 'æ¶¨è·Œåœæ•°æ®', 'æ‰€å±æ¦‚å¿µ']

# =====================================================================
# --- II. æ ¸å¿ƒå·¥å…·å‡½æ•° ---
# =====================================================================

def get_beijing_now():
    utc_now = datetime.utcnow()
    return utc_now + timedelta(hours=8)

def get_closest_trade_date():
    print(f"ğŸ“… æ­£åœ¨è®¡ç®—ç›®æ ‡äº¤æ˜“æ—¥...")
    if not os.path.exists(CALENDAR_PATH):
        print(f"âŒ é”™è¯¯ï¼šäº¤æ˜“æ—¥å†æ–‡ä»¶æœªæ‰¾åˆ°: {CALENDAR_PATH}")
        return None
    try:
        jyrl = pd.read_csv(CALENDAR_PATH)
        if 'date' not in jyrl.columns:
             jyrl['date'] = pd.to_datetime(jyrl['trade_date']).dt.date
    except Exception as e:
        print(f"âŒ è¯»å–äº¤æ˜“æ—¥å†å¤±è´¥: {e}")
        return None

    now_date = get_beijing_now().date()
    filtered_trades = jyrl[jyrl['date'] <= now_date].sort_values(by='date', ascending=False)
    
    if filtered_trades.empty:
        return None
    
    target_date_str = filtered_trades.iloc[0]['trade_date']
    print(f"âœ… é€‰å®šå¤„ç†æ—¥æœŸ: {target_date_str}")
    return target_date_str

def format_code(code):
    if pd.isna(code): return ""
    s = str(code)
    res = re.findall(r'\d+', s)
    return res[0].zfill(6) if res else ""

def clean_old_files(backup_dir, keep_days=30):
    if not os.path.exists(backup_dir): return
    files = glob.glob(os.path.join(backup_dir, "*.csv"))
    files.sort(reverse=True) 
    if len(files) > keep_days:
        print(f"ğŸ§¹ æ¸…ç† {os.path.basename(backup_dir)}: ä¿ç•™æœ€æ–° {keep_days} ä¸ª")
        for f in files[keep_days:]:
            try: os.remove(f)
            except: pass

# =====================================================================
# --- III. ä¸‹è½½é€»è¾‘ (ä¼˜åŒ–ï¼šåˆ—è¿‡æ»¤) ---
# =====================================================================

def download_task(date, config_name, config):
    max_retries = 3 
    backup_dir = config['backup_dir']
    question_suffix = config['question_suffix']
    data_threshold = config['data_threshold']
    keep_cols = config.get('keep_cols', [])
    
    os.makedirs(backup_dir, exist_ok=True)
    date_chinese = f"{date[:4]}å¹´{int(date[5:7])}æœˆ{int(date[8:10])}æ—¥"
    question = f'{date_chinese}{question_suffix}'
    save_path = os.path.join(backup_dir, f'{date}.csv')
    
    for retry in range(max_retries):
        try:
            time.sleep(random.uniform(*config['query_delay_range']))
            res = pywencai.get(question=question, loop=True)

            if res is None:
                print(f"  âš ï¸ [{config_name}] è¿”å›ç©ºï¼Œé‡è¯• {retry+1}")
                continue

            if len(res) < data_threshold:
                print(f"  âš ï¸ [{config_name}] æ•°æ®å°‘ ({len(res)})ï¼Œé‡è¯• {retry+1}")
                continue

            # --- å…³é”®ä¿®æ”¹ï¼šåªä¿ç•™éœ€è¦çš„åˆ— ---
            if keep_cols:
                # æ‰¾å‡º DataFrame ä¸­å­˜åœ¨çš„ã€ä¸”åœ¨ä¿ç•™åˆ—è¡¨ä¸­çš„åˆ—
                existing_cols = [c for c in keep_cols if c in res.columns]
                
                # æœ‰æ—¶å€™ pywencai è¿”å›çš„åˆ—åä¼šæœ‰ç»†å¾®å·®åˆ«ï¼ˆæ¯”å¦‚"æ¶¨åœåŸå› ç±»åˆ«"å˜æˆ"æ¶¨åœåŸå› ç±»åˆ«[20250101]"ï¼‰
                # è¿™é‡Œåšä¸€ä¸ªæ¨¡ç³ŠåŒ¹é…è¡¥å……
                for col in res.columns:
                    for target in keep_cols:
                        if target in col and col not in existing_cols:
                            # é¿å…æŠŠ unrelated column æ¯”å¦‚ 'è‚¡ç¥¨ä»£ç .1' åŠ è¿›æ¥
                            if len(col) < len(target) + 15: 
                                existing_cols.append(col)
                
                # å»é‡
                existing_cols = list(set(existing_cols))
                
                if existing_cols:
                    res = res[existing_cols]

            # ä¿å­˜
            res.to_csv(save_path, index=False, encoding='utf-8-sig')
            print(f"  âœ… [{config_name}] ä¸‹è½½æˆåŠŸ: {len(res)} æ¡")
            clean_old_files(backup_dir)
            return True

        except Exception as e:
            print(f"  âŒ [{config_name}] å¼‚å¸¸: {str(e)[:50]}")
            time.sleep(5) 

    return False

def run_downloads(target_date):
    print(f"\nğŸš€ [ç¬¬ä¸€æ­¥] ä¸‹è½½æ•°æ® ({target_date})...")
    success_count = 0
    with ThreadPoolExecutor(max_workers=1) as executor:
        futures = {}
        for name in SCENARIO_ORDER:
            config = DOWNLOAD_CONFIGS[name]
            futures[executor.submit(download_task, target_date, name, config)] = name
        for future in futures:
            if future.result(): success_count += 1
    return success_count > 0 

# =====================================================================
# --- IV. æ•°æ®åˆæˆé€»è¾‘ ---
# =====================================================================

def process_and_merge_files():
    print(f"\nğŸš€ [ç¬¬äºŒæ­¥] åˆæˆ [æ‰€å±æ¦‚å¿µ.csv]...")
    daily_dir = DOWNLOAD_CONFIGS['æ‰€å±æ¦‚å¿µ']['backup_dir']
    closing_dir = DOWNLOAD_CONFIGS['æ”¶ç›˜æ•°æ®']['backup_dir']
    zt_dir = DOWNLOAD_CONFIGS['æ¶¨è·Œåœæ•°æ®']['backup_dir']

    # --- 1. æ¦‚å¿µ ---
    # --- 1. æ¦‚å¿µ ---
    c_list = []
    if os.path.exists(daily_dir):
        files = sorted(glob.glob(os.path.join(daily_dir, "*.csv")), reverse=True)[:10]
        for f in files:
            try:
                # 1. è¯»å–æ•°æ®
                try: df = pd.read_csv(f, encoding='gbk', dtype=str)
                except: df = pd.read_csv(f, encoding='utf-8-sig', dtype=str)
                
                # 2. ç»Ÿä¸€â€œè‚¡ç¥¨ä»£ç â€åˆ—å
                # è‡ªåŠ¨å¯»æ‰¾åŒ…å«'ä»£ç 'å­—æ ·çš„åˆ—ï¼Œæˆ–è€…ç›´æ¥æŒ‡å®š
                if 'ä»£ç ' in df.columns and 'è‚¡ç¥¨ä»£ç ' not in df.columns:
                    df.rename(columns={'ä»£ç ': 'è‚¡ç¥¨ä»£ç '}, inplace=True)
                
                # 3. å¤„ç†â€œè‚¡ç¥¨ç®€ç§°â€ (å¦‚æœä¸å­˜åœ¨åˆ™å¡«ç©ºï¼Œé˜²æ­¢æŠ¥é”™)
                if 'è‚¡ç¥¨ç®€ç§°' not in df.columns and 'è‚¡ç¥¨åç§°' not in df.columns:
                    df['è‚¡ç¥¨ç®€ç§°'] = '' # æˆ–è€…æ ¹æ®ä¸šåŠ¡é€»è¾‘å»å…¶ä»–æ–‡ä»¶æ‰¾ç®€ç§°
                elif 'è‚¡ç¥¨åç§°' in df.columns:
                    df.rename(columns={'è‚¡ç¥¨åç§°': 'è‚¡ç¥¨ç®€ç§°'}, inplace=True)
    
    # --- å…³é”®ä¿®æ”¹ç‚¹ 3: ç²¾ç¡®åŒ¹é…â€œæ‰€å±æ¦‚å¿µâ€ ---
                # æ’é™¤æ‰â€œæ‰€å±æ¦‚å¿µæ•°é‡â€ï¼Œåªæ‰¾åå­—å®Œå…¨ç­‰äºâ€œæ‰€å±æ¦‚å¿µâ€çš„åˆ—
                concept_col = next((c for c in df.columns if c == 'æ‰€å±æ¦‚å¿µ'), None)
                
                if concept_col and 'è‚¡ç¥¨ä»£ç ' in df.columns:
                    # æ ‡å‡†åŒ–åˆ—å
                    df.rename(columns={concept_col: 'æ‰€å±æ¦‚å¿µ'}, inplace=True)
                    df['è‚¡ç¥¨ä»£ç '] = df['è‚¡ç¥¨ä»£ç '].apply(format_code)
                    df['file_date'] = os.path.basename(f)[:10]
                    
                    # åªé€‰å–å­˜åœ¨çš„åˆ—ï¼Œé¿å… KeyError
                    available_cols = [c for c in ['è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨ç®€ç§°', 'æ‰€å±æ¦‚å¿µ', 'file_date'] if c in df.columns]
                    c_list.append(df[available_cols])
                    
            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ–‡ä»¶ {os.path.basename(f)} æ—¶å‡ºé”™: {e}") # å»ºè®®æ‰“å°é”™è¯¯ï¼Œæ–¹ä¾¿è°ƒè¯•
                pass

    df_c = pd.concat(c_list, ignore_index=True)
    df_c = df_c.sort_values(by=['è‚¡ç¥¨ä»£ç ', 'file_date'], ascending=[True, False]).drop_duplicates('è‚¡ç¥¨ä»£ç ')
    
    # --- 2. è¡Œä¸š ---
    df_i = pd.DataFrame()
    if os.path.exists(closing_dir):
        files = sorted(glob.glob(os.path.join(closing_dir, "*.csv")), reverse=True)[:5]
        i_list = []
        for f in files:
            try:
                try: df = pd.read_csv(f, encoding='gbk', dtype=str)
                except: df = pd.read_csv(f, encoding='utf-8-sig', dtype=str)
                
                df.rename(columns={'ä»£ç ': 'è‚¡ç¥¨ä»£ç '}, inplace=True)
                ind_col = next((c for c in df.columns if 'æ‰€å±åŒèŠ±é¡ºè¡Œä¸š' in c), None)
                
                if ind_col:
                    df.rename(columns={ind_col: 'æ‰€å±è¡Œä¸š'}, inplace=True)
                    df['è‚¡ç¥¨ä»£ç '] = df['è‚¡ç¥¨ä»£ç '].apply(format_code)
                    df['file_date'] = os.path.basename(f)[:10]
                    i_list.append(df[['è‚¡ç¥¨ä»£ç ', 'æ‰€å±è¡Œä¸š', 'file_date']])
            except: pass
        if i_list:
            df_i = pd.concat(i_list).sort_values(by=['file_date'], ascending=False).drop_duplicates('è‚¡ç¥¨ä»£ç ')

    # --- 3. æ¶¨åœåŸå›  ---
    reason_dict = {}
    if os.path.exists(zt_dir):
        files = sorted(glob.glob(os.path.join(zt_dir, "*.csv")), reverse=True)[:30]
        for f in files:
            try:
                try: df = pd.read_csv(f, encoding='gbk', dtype=str)
                except: df = pd.read_csv(f, encoding='utf-8-sig', dtype=str)
                
                col_code = next((c for c in ['code', 'è‚¡ç¥¨ä»£ç ', 'ä»£ç '] if c in df.columns), None)
                reason_cols = [c for c in df.columns if 'æ¶¨åœåŸå› ç±»åˆ«' in c]
                
                if col_code and reason_cols:
                    for _, row in df.iterrows():
                        code = format_code(row[col_code])
                        reason = str(row[reason_cols[0]])
                        if reason and reason not in ['nan', 'None', '-']:
                            parts = [p.strip() for p in reason.split('+') if p.strip()]
                            if code not in reason_dict: reason_dict[code] = []
                            reason_dict[code].extend(parts)
            except: pass

    processed_reasons = {k: "+".join(list(dict.fromkeys(v))) for k, v in reason_dict.items()}

    # --- 4. åˆå¹¶ ---
    print("  ğŸ”„ æ‰§è¡Œåˆå¹¶...")
    if not df_i.empty:
        final_df = pd.merge(df_c[['è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨ç®€ç§°', 'æ‰€å±æ¦‚å¿µ']], 
                            df_i[['è‚¡ç¥¨ä»£ç ', 'æ‰€å±è¡Œä¸š']], 
                            on='è‚¡ç¥¨ä»£ç ', how='left')
    else:
        final_df = df_c[['è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨ç®€ç§°', 'æ‰€å±æ¦‚å¿µ']].copy()
        final_df['æ‰€å±è¡Œä¸š'] = ''

    final_df['å†å²æ¶¨åœåŸå› ç±»åˆ«'] = final_df['è‚¡ç¥¨ä»£ç '].map(processed_reasons).fillna('')
    final_df['code'] = final_df['è‚¡ç¥¨ä»£ç ']

    cols = ['è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨ç®€ç§°', 'æ‰€å±æ¦‚å¿µ', 'å†å²æ¶¨åœåŸå› ç±»åˆ«', 'æ‰€å±è¡Œä¸š', 'code']
    for c in cols:
        if c not in final_df.columns: final_df[c] = ''
    final_df = final_df[cols]

    os.makedirs(os.path.dirname(CONCEPT_OUTPUT_PATH), exist_ok=True)
    final_df.to_csv(CONCEPT_OUTPUT_PATH, index=False, encoding='utf-8-sig')
    print(f"âœ… æ›´æ–°æˆåŠŸ: {len(final_df)} æ¡")

if __name__ == '__main__':
    target_date = get_closest_trade_date()
    if target_date:
        if run_downloads(target_date):
            process_and_merge_files()
        else:
            sys.exit(1)
    else:
        sys.exit(0)